import numpy as np
from pathlib import Path
from typing import Literal
from copy import deepcopy
import inspect

from refAV.utils import (
    cache_manager, composable, composable_relational, get_cuboid_from_uuid,
    get_ego_SE3, get_ego_uuid,
    get_map, get_nth_pos_deriv, get_nth_radial_deriv,
    get_nth_yaw_deriv, get_pedestrian_crossings,
     get_pos_within_lane, get_road_side, get_scenario_lanes,
    get_scenario_timestamps, get_timestamps, get_uuids_of_category,
    get_semantic_lane, cuboid_distance, to_scenario_dict,
    unwrap_func, dilate_convex_polygon, polygons_overlap, is_point_in_polygon,
    swap_keys_and_listed_values, has_free_will, at_stop_sign_, remove_empty_branches,
    scenario_at_timestamps, reconstruct_track_dict, create_mining_pkl,
    post_process_scenario, get_object
)


@composable_relational
@cache_manager.create_cache('has_objects_in_relative_direction')
def has_objects_in_relative_direction(
    track_candidates:dict,
    related_candidates:dict, 
    log_dir:Path, 
    direction:Literal["forward", "backward", "left", "right"], 
    min_number:int=1, 
    max_number:int=np.inf, 
    within_distance:float=50, 
    lateral_thresh:float=np.inf) -> dict:
    """
    Identifies tracked objects with at least the minimum number of related candidates in the specified direction.
    If the minimum number is met, will create relationships equal to the max_number of closest objects. 

    Args:
        track_candidates: Tracks to analyze (scenario dictionary).
        related_candidates: Candidates to check for in direction (scenario dictionary).
        log_dir: Path to scenario logs.
        direction: Direction to analyze from the track's point of view ('forward', 'backward', 'left', 'right').
        min_number: Minimum number of objects to identify in the direction per timestamp. Defaults to 1.
        max_number: Maximum number of objects to identify in the direction per timestamp. Defaults to infinity.
        within_distance: Maximum distance for considering an object in the direction. Defaults to infinity.
        lateral_thresh: Maximum lateral distance the related object can be from the sides of the tracked object. Defaults to infinity.

    Returns:
        dict: 
            A scenario dictionary where keys are track UUIDs and values are dictionaries containing related candidate UUIDs 
            and lists of timestamps when the condition is met for that relative direction.

    Example:
        vehicles_with_peds_in_front = has_objects_in_relative_direction(vehicles, pedestrians, log_dir, direction='forward', min_number=2)
    """

    track_uuid = track_candidates
    candidate_uuids = related_candidates

    if track_uuid == get_ego_uuid(log_dir):
        #Ford Fusion dimensions offset from ego_coordinate frame
        track_width = 1
        track_front = 4.877/2 + 1.422
        track_back = 4.877 - (4.877/2 + 1.422)
    else:
        track_cuboid = get_cuboid_from_uuid(track_uuid, log_dir)
        track_width = track_cuboid.width_m/2
        track_front = track_cuboid.length_m/2
        track_back = -track_cuboid.length_m/2
    
    timestamps_with_objects = []
    objects_in_relative_direction = {}
    in_direction_dict = {}

    for candidate_uuid in candidate_uuids:
        if candidate_uuid == track_uuid:
            continue

        pos, timestamps = get_nth_pos_deriv(candidate_uuid, 0, log_dir, coordinate_frame=track_uuid)

        for i in range(len(timestamps)):

            if direction == 'left' and pos[i, 1]>track_width and (track_back-lateral_thresh<pos[i,0]<track_front+lateral_thresh)  \
            or direction == 'right' and pos[i, 1]<-track_width and (track_back-lateral_thresh<pos[i,0]<track_front+lateral_thresh)\
            or direction == 'forward' and pos[i,0]>track_front and (-track_width-lateral_thresh<pos[i,1]<track_width+lateral_thresh)\
            or direction == 'backward' and pos[i,0]<track_back and (-track_width-lateral_thresh<pos[i,1]<track_width+lateral_thresh):  
                if not in_direction_dict.get(timestamps[i], None):
                    in_direction_dict[timestamps[i]] = []
                
                distance = cuboid_distance(track_uuid, candidate_uuid, log_dir, timestamp=timestamps[i])
                in_direction_dict[timestamps[i]].append((candidate_uuid, distance))

    for timestamp, objects in in_direction_dict.items():
        sorted_objects = sorted(objects, key=lambda row: row[1])

        count = 0
        true_uuids = []
        for candidate_uuid, distance in sorted_objects:
            if distance <= within_distance and count < max_number:
                count += 1
                true_uuids.append(candidate_uuid)

        if count >= min_number:
            for true_uuid in true_uuids:
                if true_uuid not in objects_in_relative_direction:
                    objects_in_relative_direction[true_uuid] = []
                objects_in_relative_direction[true_uuid].append(timestamp)
                timestamps_with_objects.append(timestamp)

    return timestamps_with_objects, objects_in_relative_direction


@cache_manager.create_cache('get_objects_in_relative_direction')
def get_objects_in_relative_direction(
    track_candidates:dict,
    related_candidates:dict, 
    log_dir:Path, 
    direction:Literal["forward", "backward", "left", "right"], 
    min_number:int=0, 
    max_number:int=np.inf, 
    within_distance:float=50, 
    lateral_thresh:float=np.inf)->dict:
    """
    Returns a scenario dictionary of the related candidates that are in the relative direction of the track candidates.
    

    Args:
        track_candidates: Tracks  (scenario dictionary).
        related_candidates: Candidates to check for in direction (scenario dictionary).
        log_dir: Path to scenario logs.
        direction: Direction to analyze from the track's point of view ('forward', 'backward', 'left', 'right').
        min_number: Minimum number of objects to identify in the direction per timestamp. Defaults to 0.
        max_number: Maximum number of objects to identify in the direction per timestamp. Defaults to infinity.
        within_distance: Maximum distance for considering an object in the direction. Defaults to infinity.
        lateral_thresh: Maximum lateral distance the related object can be from the sides of the tracked object. Lateral distance is 
        distance is the distance from the sides of the object that are parallel to the specified direction. Defaults to infinity.

    Returns:
        dict: 
            A scenario dictionary where keys are track UUIDs and values are dictionaries containing related candidate UUIDs 
            and lists of timestamps when the condition is met for that relative direction.

    Example:
        peds_in_front_of_vehicles = get_objects_in_relative_direction(vehicles, pedestrians, log_dir, direction='forward', min_number=2)
    """
    
    tracked_objects = \
    reverse_relationship(has_objects_in_relative_direction)(track_candidates, related_candidates, log_dir, direction,
        min_number=min_number, max_number=max_number, within_distance=within_distance, lateral_thresh=lateral_thresh)

    return tracked_objects


def get_objects_of_category(log_dir, category)->dict:
    """
    Returns all objects from a given category from the log annotations. This method accepts the 
    super-categories "ANY" and "VEHICLE".

    Args:
        log_dir: Path to the directory containing scenario logs and data.
        category: the category of objects to return

    Returns: 
        dict: A scenario dict that where keys are the unique id (uuid) of the object and values 
        are the list of timestamps the object is in view of the ego-vehicle.

    Example:
        trucks = get_objects_of_category(log_dir, category='TRUCK')
    """
    return to_scenario_dict(get_uuids_of_category(log_dir, category), log_dir)


@composable
def is_category(track_candidates:dict, log_dir:Path, category:str):
    """
    Returns all objects from a given category from track_candidates dict. This method accepts the 
    super-categories "ANY" and "VEHICLE".

    Args:
        track_candidates: The scenario dict containing the objects to filter down
        log_dir: Path to the directory containing scenario logs and data.
        category: the category of objects to return

    Returns: 
        dict: A scenario dict that where keys are the unique id of the object of the given category and values 
        are the list of timestamps the object is in view of the ego-vehicle.

    Example:
        box_trucks = is_category(vehicles, log_dir, category='BOX_TRUCK')
    """


    track_uuid = track_candidates
    if track_uuid in get_uuids_of_category(log_dir, category):
        non_composable_get_object = unwrap_func(get_object)
        return non_composable_get_object(track_uuid, log_dir)
    else:
        return []


@composable
@cache_manager.create_cache('turning')
def turning(
    track_candidates: dict, 
    log_dir:Path,
    direction:Literal["left", "right", None]=None)->dict:
    """
    Returns objects that are turning in the given direction. 

    Args:
        track_candidates: The objects you want to filter from (scenario dictionary).
        log_dir: Path to scenario logs.
        direction: The direction of the turn, from the track's point of view ('left', 'right', None).

    Returns:
        dict: 
            A filtered scenario dictionary where:
            - Keys are track UUIDs that meet the turning criteria.
            - Values are nested dictionaries containing timestamps.

    Example:
        turning_left = turning(vehicles, log_dir, direction='left')
    """
    track_uuid = track_candidates

    if direction and direction != 'left' and direction != 'right':
        direction = None
        print("Specified direction must be 'left', 'right', or None. Direction set to \
              None automatically.")
    
    TURN_ANGLE_THRESH = 45 #degrees 
    ANG_VEL_THRESH = 5 #deg/s

    ang_vel, timestamps = get_nth_yaw_deriv(track_uuid, 1, log_dir, coordinate_frame='self', in_degrees=True)

    turn_dict = {'left': [], 'right':[]}

    start_index = 0
    end_index = start_index
    
    while start_index < len(timestamps)-1:
        #Check if the object is continuing to turn in the same direction
        if ((ang_vel[start_index] > 0 and ang_vel[end_index] > 0 
        or ang_vel[start_index] < 0 and ang_vel[end_index] < 0) 
        and end_index < len(timestamps)-1):
            end_index += 1
        else:
            #Check if the object's angle has changed enough to define a turn
            s_per_timestamp = float(timestamps[1] - timestamps[0])/1E9
            if np.sum(ang_vel[start_index:end_index+1]*s_per_timestamp) > TURN_ANGLE_THRESH:
                turn_dict['left'].extend(timestamps[start_index:end_index+1])
            elif np.sum(ang_vel[start_index:end_index+1]*s_per_timestamp) < -TURN_ANGLE_THRESH:
                turn_dict['right'].extend(timestamps[start_index:end_index+1])
            elif (unwrap_func(near_intersection)(track_uuid, log_dir) 
            and (start_index == 0 and unwrap_func(near_intersection)(track_uuid, log_dir)[0] == timestamps[0]
                or end_index == len(timestamps)-1 and unwrap_func(near_intersection)(track_uuid, log_dir)[-1] == timestamps[-1])):

                if (((start_index==0 and ang_vel[start_index] > ANG_VEL_THRESH) 
                    or (end_index==len(timestamps)-1 and ang_vel[end_index] > ANG_VEL_THRESH))
                and np.mean(ang_vel[start_index:end_index+1]) > ANG_VEL_THRESH
                and np.sum(ang_vel[start_index:end_index+1]*s_per_timestamp) > TURN_ANGLE_THRESH/3):
                    turn_dict['left'].extend(timestamps[start_index:end_index+1])
                elif (((start_index==0 and ang_vel[start_index] < -ANG_VEL_THRESH) 
                    or (end_index==len(timestamps)-1 and ang_vel[end_index] < -ANG_VEL_THRESH))
                and np.mean(ang_vel[start_index:end_index+1]) < -ANG_VEL_THRESH
                and np.sum(ang_vel[start_index:end_index+1]*s_per_timestamp) < -TURN_ANGLE_THRESH/3):
                    turn_dict['right'].extend(timestamps[start_index:end_index+1])

            start_index = end_index
            end_index += 1 
    
    if direction:
        return turn_dict[direction]
    else:
        return turn_dict['left'] + turn_dict['right']  
    

@composable
@cache_manager.create_cache('changing_lanes')
def changing_lanes(
    track_candidates:dict, 
    log_dir:Path,
    direction:Literal["left", "right", None]=None) -> dict:
    """
    Identifies lane change events for tracked objects in a scenario.

    Args:
        track_candidates: The tracks to analyze (scenario dictionary).
        log_dir: Path to scenario logs.
        direction: The direction of the lane change. None indicates tracking either left or right lane changes ('left', 'right', None).

    Returns:
        dict: 
            A filtered scenario dictionary where:
            Keys are track UUIDs that meet the lane change criteria.
            Values are nested dictionaries containing timestamps and related data.

    Example:
        left_lane_changes = changing_lanes(vehicles, log_dir, direction='left')
    """
    track_uuid = track_candidates

    if direction is not None and direction != 'right' and direction != 'left':
        print("Direction must be 'right', 'left', or None.")
        print("Setting direction to None.")
        direction = None

    COS_SIMILARITY_THRESH = .5 #vehicle must be headed in a direction at most 45 degrees from the direction of the lane boundary
    SIDEWAYS_VEL_THRESH = .1 #m/s

    lane_traj = get_scenario_lanes(track_uuid, log_dir)
    positions, timestamps = get_nth_pos_deriv(track_uuid, 0, log_dir)
    velocities, timestamps = get_nth_pos_deriv(track_uuid, 1, log_dir)
    #Each index stored in dict indicates the exact timestep where the track crossed lanes
    lane_changes_exact = {'left': [], 'right':[]}
    for i in range(1, len(timestamps)):
        prev_lane = lane_traj[timestamps[i-1]]
        cur_lane = lane_traj[timestamps[i]]

        if prev_lane and cur_lane and abs(velocities[i,1]) >= SIDEWAYS_VEL_THRESH:
            if prev_lane.right_neighbor_id == cur_lane.id:

                #caclulate lane orientation
                closest_waypoint_idx = np.argmin(np.linalg.norm(prev_lane.right_lane_boundary.xyz[:,:2]-positions[i,:2], axis=1))
                start_idx = max(0, closest_waypoint_idx-1)
                end_idx = min(len(prev_lane.right_lane_boundary.xyz)-1, closest_waypoint_idx + 1)
                lane_boundary_direction = prev_lane.right_lane_boundary.xyz[end_idx,:2] - prev_lane.right_lane_boundary.xyz[start_idx,:2]
                lane_boundary_direction /= np.linalg.norm(lane_boundary_direction + 1e-8)
                track_direction = velocities[i,:2] / np.linalg.norm(velocities[i,:2])
                lane_change_cos_similarity = abs(np.dot(lane_boundary_direction, track_direction))

                if lane_change_cos_similarity >= COS_SIMILARITY_THRESH:
                    lane_changes_exact['right'].append(i)
            elif prev_lane.left_neighbor_id == cur_lane.id:
                #caclulate lane orientation
                closest_waypoint_idx = np.argmin(np.linalg.norm(prev_lane.left_lane_boundary.xyz[:,:2]-positions[i,:2], axis=1))

                start_idx = min(0, closest_waypoint_idx - 1)
                end_idx = min(len(prev_lane.left_lane_boundary.xyz)-1, closest_waypoint_idx + 1)
                lane_boundary_direction = prev_lane.left_lane_boundary.xyz[end_idx,:2] - prev_lane.left_lane_boundary.xyz[start_idx,:2]
                lane_boundary_direction /= np.linalg.norm(lane_boundary_direction + 1e-8)
                track_direction = velocities[i,:2] / np.linalg.norm(velocities[i,:2])
                lane_change_cos_similarity = abs(np.dot(lane_boundary_direction, track_direction))

                if lane_change_cos_similarity >= COS_SIMILARITY_THRESH:
                    lane_changes_exact['left'].append(i)

    lane_changes = {'left': [], 'right':[]}

    for index in lane_changes_exact['left']:
        lane_change_start = index - 1
        lane_change_end = index

        while lane_change_start > 0:
            _, pos_along_width0 = get_pos_within_lane(positions[lane_change_start], lane_traj[timestamps[lane_change_start]])
            _, pos_along_width1 = get_pos_within_lane(positions[lane_change_start+1], lane_traj[timestamps[lane_change_start+1]])

            if (pos_along_width0 and pos_along_width1 and pos_along_width0 > pos_along_width1) or lane_change_start == index-1:
                lane_changes['left'].append(timestamps[lane_change_start])
                lane_change_start -= 1
            else:
                break
            
        while lane_change_end < len(timestamps):
            _, pos_along_width0 = get_pos_within_lane(positions[lane_change_end-1], lane_traj[timestamps[lane_change_end-1]])
            _, pos_along_width1 = get_pos_within_lane(positions[lane_change_end], lane_traj[timestamps[lane_change_end]])

            if (pos_along_width0 and pos_along_width1 and pos_along_width0 > pos_along_width1) or lane_change_end == index:
                lane_changes['left'].append(timestamps[lane_change_end])
                lane_change_end += 1
            else:
                break
    
    for index in lane_changes_exact['right']:
        lane_change_start = index - 1
        lane_change_end = index

        while lane_change_start > 0:
            _, pos_along_width0 = get_pos_within_lane(positions[lane_change_start], lane_traj[timestamps[lane_change_start]])
            _, pos_along_width1 = get_pos_within_lane(positions[lane_change_start+1], lane_traj[timestamps[lane_change_start+1]])

            if pos_along_width0 and pos_along_width1 and pos_along_width0 < pos_along_width1 or lane_change_start == index-1:
                lane_changes['right'].append(timestamps[lane_change_start])
                lane_change_start -= 1
            else:
                break
            
        while lane_change_end < len(timestamps):
            _, pos_along_width0 = get_pos_within_lane(positions[lane_change_end-1], lane_traj[timestamps[lane_change_end-1]])
            _, pos_along_width1 = get_pos_within_lane(positions[lane_change_end], lane_traj[timestamps[lane_change_end]])

            if pos_along_width0 and pos_along_width1 and pos_along_width0 < pos_along_width1 or lane_change_end == index:
                lane_changes['right'].append(timestamps[lane_change_end])
                lane_change_end += 1
            else:
                break

    if direction:
        lane_changing_timestamps = lane_changes[direction]
    else:
        lane_changing_timestamps = sorted(list(set(lane_changes['left'] + (lane_changes['right']))))

    turning_timestamps = unwrap_func(turning)(track_uuid, log_dir)
    return sorted(list(set(lane_changing_timestamps).difference(set(turning_timestamps))))


@composable
@cache_manager.create_cache('has_lateral_acceleration')
def has_lateral_acceleration(
    track_candidates:dict,
    log_dir:Path,
    min_accel=-np.inf,
    max_accel=np.inf) -> dict:
    """
    Objects with a lateral acceleartion between the minimum and maximum thresholds. 
    Most objects with a high lateral acceleration are turning. Postive values indicate accelaration
    to the left while negative values indicate acceleration to the right. 

    Args:
        track_candidates: The tracks to analyze (scenario dictionary).
        log_dir: Path to scenario logs.
        direction: The direction of the lane change. None indicates tracking either left or right lane changes ('left', 'right', None).

    Returns:
        dict: 
            A filtered scenario dictionary where:
            Keys are track UUIDs that meet the lane change criteria.
            Values are nested dictionaries containing timestamps and related data.

    Example:
        jerking_left = has_lateral_acceleration(non_turning_vehicles, log_dir, min_accel=2)
    """
    track_uuid = track_candidates

    hla_timestamps = []
    accelerations, timestamps = get_nth_pos_deriv(track_uuid, 2, log_dir, coordinate_frame='self')
    for i, accel in enumerate(accelerations):
        if min_accel <= accel[1] <= max_accel: #m/s^2
            hla_timestamps.append(timestamps[i])

    if unwrap_func(stationary)(track_candidates, log_dir):
        return []

    return hla_timestamps


@composable_relational
@cache_manager.create_cache('facing_toward')
def facing_toward(
    track_candidates:dict,
    related_candidates:dict,
    log_dir:Path,
    within_angle:float=22.5,
    max_distance:float=50)->dict:
    """
    Identifies objects in track_candidates that are facing toward objects in related candidates.
    The related candidate must lie within a region lying within within_angle degrees on either side the track-candidate's forward axis.

    Args:
        track_candidates: The tracks that could be heading toward another tracks
        related_candidates: The objects to analyze to see if the track_candidates are heading toward
        log_dir:  Path to the directory containing scenario logs and data.
        fov: The field of view of the track_candidates. The related candidate must lie within a region lying 
            within fov/2 degrees on either side the track-candidate's forward axis.
        max_distance: The maximum distance a related_candidate can be away to be considered by 

    Returns:
        A filtered scenario dict that contains the subset of track candidates heading toward at least one of the related candidates.

    Example:
        pedestrian_facing_away = scenario_not(facing_toward)(pedestrian, ego_vehicle, log_dir, within_angle=180)
    """

    track_uuid = track_candidates
    facing_toward_timestamps = []
    facing_toward_objects = {}

    for candidate_uuid in related_candidates:

        if candidate_uuid == track_uuid:
            continue

        traj, timestamps = get_nth_pos_deriv(candidate_uuid, 0, log_dir, coordinate_frame=track_uuid)
        for i, timestamp in enumerate(timestamps):

            angle = np.rad2deg(np.arctan2(traj[i, 1],  traj[i,0]))
            distance = cuboid_distance(track_uuid, candidate_uuid, log_dir, timestamp=timestamp)

            if np.abs(angle) <= within_angle and distance <= max_distance:
                facing_toward_timestamps.append(timestamp)

                if candidate_uuid not in facing_toward_objects:
                    facing_toward_objects[candidate_uuid] = []
                facing_toward_objects[candidate_uuid].append(timestamp)

    return facing_toward_timestamps, facing_toward_objects


@composable_relational
@cache_manager.create_cache('heading_toward')
def heading_toward(
    track_candidates:dict,
    related_candidates:dict,
    log_dir:Path,
    angle_threshold:float=22.5,
    minimum_speed:float=.5,
    max_distance:float=np.inf)->dict:
    """
    Identifies objects in track_candidates that are heading toward objects in related candidates.
    The track candidates acceleartion vector must be within the given angle threshold of the relative position vector.
    The track candidates must have a component of velocity toward the related candidate greater than the minimum_accel.

    Args:
        track_candidates: The tracks that could be heading toward another tracks
        related_candidates: The objects to analyze to see if the track_candidates are heading toward
        log_dir:  Path to the directory containing scenario logs and data.
        angle_threshold: The maximum angular difference between the velocity vector and relative position vector between
            the track candidate and related candidate.
        min_vel: The minimum magnitude of the component of velocity toward the related candidate
        max_distance: Distance in meters the related candidates can be away from the track candidate to be considered

    Returns:
        A filted scenario dict that contains the subset of track candidates heading toward at least one of the related candidates.


    Example:
        heading_toward_traffic_cone = heading_toward(vehicles, traffic_cone, log_dir)
    """

    track_uuid = track_candidates
    heading_toward_timestamps = []
    heading_toward_objects = {}

    track_vel, track_timestamps = get_nth_pos_deriv(track_uuid, 1, log_dir, coordinate_frame=track_uuid)

    for candidate_uuid in related_candidates:
        if candidate_uuid == track_uuid:
            continue

        related_pos, _ = get_nth_pos_deriv(candidate_uuid, 0, log_dir, coordinate_frame=track_uuid)
        track_radial_vel, related_timestamps = get_nth_radial_deriv(
            track_uuid, 1, log_dir, coordinate_frame=candidate_uuid)
        
        for i, timestamp in enumerate(related_timestamps):
            if timestamp not in track_timestamps:
                continue
            timestamp_vel = track_vel[track_timestamps.index(timestamp)]

            vel_direction = timestamp_vel/(np.linalg.norm(timestamp_vel) + 1e-8)
            direction_of_related = related_pos[i]/np.linalg.norm(related_pos[i] + 1e-8)
            angle = np.rad2deg(np.arccos(np.dot(vel_direction, direction_of_related)))

            if -track_radial_vel[i] >= minimum_speed and angle <= angle_threshold \
            and cuboid_distance(track_uuid, candidate_uuid, log_dir, timestamp) <= max_distance:

                heading_toward_timestamps.append(timestamp)
                if candidate_uuid not in heading_toward_objects:
                    heading_toward_objects[candidate_uuid] = []
                heading_toward_objects[candidate_uuid].append(timestamp)

    return heading_toward_timestamps, heading_toward_objects


@composable
@cache_manager.create_cache('accelerating')
def accelerating(
    track_candidates:dict,
    log_dir:Path,
    min_accel:float=.65,
    max_accel:float=np.inf)->dict:
    """
    Identifies objects in track_candidates that have a forward acceleration above a threshold.
    Values under -1 reliably indicates braking. Values over 1.0 reliably indiciates accelerating.

    Args:
        track_candidates: The tracks to analyze for acceleration (scenario dictionary)
        log_dir:  Path to the directory containing scenario logs and data.
        min_accel: The lower bound of acceleration considered
        max_accel: The upper bound of acceleration considered

    Returns:
        A filtered scenario dictionary containing the objects with an acceleration between the lower and upper bounds.

    Example:
        accelerating_motorcycles = accelerating(motorcycles, log_dir)

    """
    track_uuid = track_candidates

    acc_timestamps = []
    accelerations, timestamps = get_nth_pos_deriv(track_uuid, 2, log_dir, coordinate_frame='self')
    for i, accel in enumerate(accelerations):
        if min_accel <= accel[0] <= max_accel: #m/s^2
            acc_timestamps.append(timestamps[i])

    if unwrap_func(stationary)(track_candidates, log_dir):
        return []

    return acc_timestamps


@composable
@cache_manager.create_cache('has_velocity')
def has_velocity(
    track_candidates:dict,
    log_dir:Path, 
    min_velocity:float=.5, 
    max_velocity:float=np.inf)->dict:
    """
    Identifies objects with a velocity between the given maximum and minimum velocities in m/s.
    Stationary objects may have a velocity up to 0.5 m/s due to annotation jitter.

    Args:
        track_candidates: Tracks to analyze (scenario dictionary).
        log_dir: Path to scenario logs.
        min_velocity: Minimum velocity (m/s). Defaults to 0.5.
        max_velocity: Maximum velocity (m/s)

    Returns:
        Filtered scenario dictionary of objects meeting the velocity criteria.

    Example:
        fast_vehicles = has_min_velocity(vehicles, log_dir, min_velocity=5)
    """
    track_uuid = track_candidates

    vel_timestamps = []
    vels, timestamps = get_nth_pos_deriv(track_uuid, 1, log_dir)
    for i, vel in enumerate(vels):
        if min_velocity <= np.linalg.norm(vel) <= max_velocity: #m/s
            vel_timestamps.append(timestamps[i])

    if unwrap_func(stationary)(track_candidates, log_dir):
        return []
        
    return vel_timestamps


@composable
@cache_manager.create_cache('at_pedestrian_crossing')
def at_pedestrian_crossing(
    track_candidates:dict,
    log_dir:Path,
    within_distance:float=1)->dict:
    """
    Identifies objects that within a certain distance from a pedestrian crossing. A distance of zero indicates
    that the object is within the boundaries of the pedestrian crossing.

    Args:
        track_candidates: Tracks to analyze (scenario dictionary).
        log_dir: Path to scenario logs.
        within_distance: Distance in meters the track candidate must be from the pedestrian crossing. A distance of zero
            means that the object must be within the boundaries of the pedestrian crossing.

    Returns:
        Filtered scenario dictionary where keys are track UUIDs and values are lists of timestamps.

    Example:
        vehicles_at_ped_crossing = at_pedestrian_crossing(vehicles, log_dir)
    """
    track_uuid = track_candidates

    avm = get_map(log_dir)
    ped_crossings = avm.get_scenario_ped_crossings()

    timestamps = get_timestamps(track_uuid, log_dir)
    ego_poses = get_ego_SE3(log_dir)

    timestamps_at_object = []
    for timestamp in timestamps:
        track_cuboid = get_cuboid_from_uuid(track_uuid, log_dir, timestamp=timestamp)
        city_vertices = ego_poses[timestamp].transform_from(track_cuboid.vertices_m) 
        track_poly = np.array([city_vertices[2],city_vertices[6],city_vertices[7],city_vertices[3],city_vertices[2]])[:,:2]

        for ped_crossing in ped_crossings:
            pc_poly = ped_crossing.polygon
            pc_poly = dilate_convex_polygon(pc_poly[:,:2], distance=within_distance)
            ped_crossings = get_pedestrian_crossings(avm, track_poly)

            if polygons_overlap(track_poly, pc_poly):
                timestamps_at_object.append(timestamp)
        
    return timestamps_at_object


@composable
@cache_manager.create_cache('on_lane_type')
def on_lane_type(
    track_uuid:dict,
    log_dir,
    lane_type:Literal["BUS", "VEHICLE", "BIKE"])->dict:
    """
    Identifies objects on a specific lane type.

    Args:
        track_candidates: Tracks to analyze (scenario dictionary).
        log_dir: Path to scenario logs.
        lane_type: Type of lane to check ('BUS', 'VEHICLE', or 'BIKE').

    Returns:
        Filtered scenario dictionary where keys are track UUIDs and values are lists of timestamps.

    Example:
        vehicles_on_bus_lane = on_lane_type(vehicles, log_dir, lane_type="BUS")
    """

    scenario_lanes = get_scenario_lanes(track_uuid, log_dir)
    timestamps = scenario_lanes.keys()

    return [timestamp for timestamp in timestamps if scenario_lanes[timestamp] and scenario_lanes[timestamp].lane_type == lane_type]


@composable
@cache_manager.create_cache('near_intersection')
def near_intersection(
    track_uuid:dict,
    log_dir:Path,
    threshold:float=5)->dict:
    """
    Identifies objects within a specified threshold of an intersection in meters.

    Args:
        track_candidates: Tracks to analyze (scenario dictionary).
        log_dir: Path to scenario logs.
        threshold: Distance threshold (in meters) to define "near" an intersection.

    Returns:
        Filtered scenario dictionary where keys are track UUIDs and values are lists of timestamps.

    Example:
        bicycles_near_intersection = near_intersection(bicycles, log_dir, threshold=10.0)
    """


    traj, timestamps = get_nth_pos_deriv(track_uuid, 0, log_dir)

    avm = get_map(log_dir)
    lane_segments = avm.get_scenario_lane_segments()

    ls_polys = []
    for ls in lane_segments:
        if ls.is_intersection:
            ls_polys.append(ls.polygon_boundary)

    dilated_intersections = []
    for ls in ls_polys:
        dilated_intersections.append(dilate_convex_polygon(ls[:,:2], threshold))
    
    near_intersection_timestamps = []
    for i, pos in enumerate(traj):
        for dilated_intersection in dilated_intersections:
            if is_point_in_polygon(pos[:2], dilated_intersection):
                near_intersection_timestamps.append(timestamps[i])

    return near_intersection_timestamps


@composable
@cache_manager.create_cache('on_intersection')
def on_intersection(track_candidates:dict, log_dir:Path):
    """
    Identifies objects located on top of an road intersection.

    Args:
        track_candidates: Tracks to analyze (scenario dictionary).
        log_dir: Path to scenario logs.

    Returns:
        Filtered scenario dictionary where keys are track UUIDs and values are lists of timestamps.

    Example:
        strollers_on_intersection = on_intersection(strollers, log_dir)
    """
    track_uuid = track_candidates

    scenario_lanes = get_scenario_lanes(track_uuid, log_dir)
    timestamps = scenario_lanes.keys()

    timestamps_on_intersection = []
    for timestamp in timestamps:
        if scenario_lanes[timestamp] is not None and scenario_lanes[timestamp].is_intersection:
            timestamps_on_intersection.append(timestamp)

    return timestamps_on_intersection


@composable_relational
@cache_manager.create_cache('being_crossed_by')
def being_crossed_by(
    track_candidates:dict, 
    related_candidates:dict, 
    log_dir:Path,
    direction:Literal["forward", "backward", "left", "right"]="forward",
    in_direction:Literal['clockwise','counterclockwise','either']='either',
    forward_thresh:float=10,
    lateral_thresh:float=5)->dict:
    """
    Identifies objects that are being crossed by one of the related candidate objects. A crossing is defined as
    the related candidate's centroid crossing the half-midplane of a tracked candidate. The direction of the half-
    midplane is specified with the direction. 

    Args:
        track_candidates: Tracks to analyze .
        related_candidates: Candidates (e.g., pedestrians or vehicles) to check for crossings.
        log_dir: Path to scenario logs.
        direction: specifies the axis and direction the half midplane extends from 
        in_direction: which direction the related candidate has to cross the midplane for it to be considered a crossing
        forward_thresh: how far the midplane extends from the edge of the tracked object
        lateral_thresh: the two planes offset from the midplane. If an related candidate crosses the midplane, it will 
        continue being considered crossing until it goes past the lateral_thresh.

    Returns:
        A filtered scenario dictionary containing all of the track candidates that were crossed by 
        the related candidates given the specified constraints.

    Example:
        overtaking_on_left = being_crossed_by(moving_cars, moving_cars, log_dir, direction="left", in_direction="clockwise", forward_thresh=4)
        vehicles_crossed_by_peds = being_crossed_by(vehicles, pedestrians, log_dir)
    """
    track_uuid = track_candidates
    VELOCITY_THRESH   = .2 #m/s 

    crossings = {}
    crossed_timestamps = []
    
    track = get_cuboid_from_uuid(track_uuid, log_dir)
    forward_thresh = track.length_m/2 + forward_thresh
    left_bound = -track.width_m/2
    right_bound = track.width_m/2

    for candidate_uuid in related_candidates:
        if candidate_uuid == track_uuid:
            continue

        #Transform from city to tracked_object coordinate frame
        candidate_pos, timestamps = get_nth_pos_deriv(candidate_uuid, 0, log_dir, coordinate_frame=track_uuid, direction=direction)
        candidate_vel, timestamps = get_nth_pos_deriv(candidate_uuid, 1, log_dir, coordinate_frame=track_uuid, direction=direction)

        for i in range(1,len(candidate_pos)):
            y0 = candidate_pos[i-1, 1]
            y1 = candidate_pos[i, 1]
            y_vel = candidate_vel[i, 1]
            if ((y0<left_bound<y1 or y1<right_bound<y0 or y0<right_bound<y1 or y1<left_bound<y0)
            and abs(y_vel) > VELOCITY_THRESH) and (track.length_m/2<=candidate_pos[i,0]<=forward_thresh) \
            and candidate_uuid != track_uuid:
                
                #1 if moving right, -1 if moving left
                direction = (y1-y0)/abs(y1-y0)
                start_index = i-1
                end_index = i
                updated = True
                
                if (direction == 1 and in_direction == 'clockwise'
                or direction == -1 and in_direction == 'counterclockwise'):
                    #The object is not moving in the specified crossing direction
                    continue
        
                while updated:
                    updated = False
                    if start_index>=0 and direction*candidate_pos[start_index, 1] < lateral_thresh \
                    and direction*candidate_vel[start_index,1] > VELOCITY_THRESH:
                        if candidate_uuid not in crossings:
                            crossings[candidate_uuid] = []
                        crossings[candidate_uuid].append(timestamps[start_index])
                        crossed_timestamps.append(timestamps[start_index])
                        updated = True
                        start_index -= 1

                    if end_index < len(timestamps) and direction*candidate_pos[end_index, 1] < lateral_thresh \
                    and direction*candidate_vel[end_index, 1] > VELOCITY_THRESH:
                        if candidate_uuid not in crossings:
                            crossings[candidate_uuid] = []
                        crossings[candidate_uuid].append(timestamps[end_index])
                        crossed_timestamps.append(timestamps[end_index])
                        updated = True
                        end_index += 1

    return crossed_timestamps, crossings


@composable_relational
@cache_manager.create_cache('near_objects')
def near_objects(
    track_uuid:dict, 
    candidate_uuids:dict, 
    log_dir:Path,
    distance_thresh:float=10, 
    min_objects:int=1,
    include_self:bool=False)->dict:
    """
    Identifies timestamps when a tracked object is near a specified set of related objects.

    Args:
        track_candidates: Tracks to analyze (scenario dictionary).
        related_candidates: Candidates to check for proximity (scenario dictionary).
        log_dir: Path to scenario logs.
        distance_thresh: Maximum distance in meters a related candidate can be away to be considered "near".
        min_objects: Minimum number of related objects required to be near the tracked object.

    Returns:
        dict: 
            A scenario dictionary where:
            Keys are timestamps when the tracked object is near the required number of related objects.
            Values are lists of related candidate UUIDs present at those timestamps.

    Example:
        vehicles_near_ped_group = near_objects(vehicles, pedestrians, log_dir, min_objects=3)
    """

    if not min_objects:
        min_objects = len(candidate_uuids)

    near_objects_dict = {}
    for candidate in candidate_uuids:
        if candidate == track_uuid and not include_self:
            continue
        
        _, timestamps = get_nth_pos_deriv(candidate, 0, log_dir, coordinate_frame=track_uuid)

        for timestamp in timestamps:
            if cuboid_distance(track_uuid, candidate, log_dir, timestamp) <= distance_thresh:
                if timestamp not in near_objects_dict:
                    near_objects_dict[timestamp] = []
                near_objects_dict[timestamp].append(candidate)
        
    timestamps = []
    keys = list(near_objects_dict.keys())
    for timestamp in keys:
        if len(near_objects_dict[timestamp]) >= min_objects:
            timestamps.append(timestamp)
        else:
            near_objects_dict.pop(timestamp)

    near_objects_dict = swap_keys_and_listed_values(near_objects_dict)

    return timestamps, near_objects_dict


@composable_relational
@cache_manager.create_cache('following')
def following(
    track_uuid:dict,
    candidate_uuids:dict,
    log_dir:Path) -> dict:
    """
    Returns timestamps when the tracked object is following a lead object
    Following is defined simultaneously moving in the same direction and lane.
    """

    lead_timestamps = []
    leads = {}

    avm = get_map(log_dir)
    track_lanes = get_scenario_lanes(track_uuid, log_dir, avm=avm)
    track_vel, track_timestamps = get_nth_pos_deriv(track_uuid, 1, log_dir, coordinate_frame=track_uuid)

    track_cuboid = get_cuboid_from_uuid(track_uuid, log_dir)
    track_width = track_cuboid.width_m/2
    track_length = track_cuboid.length_m/2

    FOLLOWING_THRESH = 25 + track_length #m
    LATERAL_TRHESH = 5 #m
    HEADING_SIMILARITY_THRESH = .5 #cosine similarity

    for j, candidate in enumerate(candidate_uuids):
        if candidate == track_uuid:
            continue

        #print(f'{j}/{len(candidate_uuids)}', end='\r')
        candidate_pos, _ = get_nth_pos_deriv(candidate, 0, log_dir, coordinate_frame=track_uuid)
        candidate_vel, _ = get_nth_pos_deriv(candidate, 1, log_dir, coordinate_frame=track_uuid) 
        candidate_yaw, timestamps = get_nth_yaw_deriv(candidate, 0, log_dir, coordinate_frame=track_uuid)
        candidate_lanes = get_scenario_lanes(candidate, log_dir, avm=avm)

        overlap_track_vel = track_vel[np.isin(track_timestamps, timestamps)]
        candidate_heading_similarity = np.zeros(len(timestamps))

        candidate_cuboid = get_cuboid_from_uuid(track_uuid, log_dir)
        candidate_width = candidate_cuboid.width_m/2

        for i in range(len(timestamps)):

            if np.linalg.norm(candidate_vel[i]) > .5:
                candidate_heading = candidate_vel[i, :2]/np.linalg.norm(candidate_vel[i,:2] + 1e-8)
            else:
                candidate_heading = np.array([np.cos(candidate_yaw[i]), np.sin(candidate_yaw[i])])

            if np.linalg.norm(overlap_track_vel[i]) > .5:
                track_heading = overlap_track_vel[i, :2]/np.linalg.norm(overlap_track_vel[i,:2] + 1e-8)  
            else:
                #Coordinates are in track_coordinate frame.
                track_heading = np.array([1,0])

            candidate_heading_similarity[i] = np.dot(track_heading, candidate_heading)

        for i in range(len(timestamps)):
            if track_lanes[timestamps[i]] and candidate_lanes[timestamps[i]] \
            and (((track_lanes[timestamps[i]].id == candidate_lanes[timestamps[i]].id \
                or candidate_lanes[timestamps[i]].id in track_lanes[timestamps[i]].successors) \
                and track_length<candidate_pos[i, 0]<FOLLOWING_THRESH and -LATERAL_TRHESH<candidate_pos[i,1]<LATERAL_TRHESH \
                and candidate_heading_similarity[i] > HEADING_SIMILARITY_THRESH)\
            or (track_lanes[timestamps[i]].left_neighbor_id == candidate_lanes[timestamps[i]].id 
                or track_lanes[timestamps[i]].right_neighbor_id == candidate_lanes[timestamps[i]].id) \
                and track_length<candidate_pos[i, 0]<FOLLOWING_THRESH 
                and (-track_width<=candidate_pos[i,1]+candidate_width<=track_width or -track_width<=candidate_pos[i,1]-candidate_width<=track_width)\
                and candidate_heading_similarity[i] > HEADING_SIMILARITY_THRESH):

                if candidate not in leads:
                    leads[candidate] = []
                leads[candidate].append(timestamps[i]) 
                lead_timestamps.append(timestamps[i]) 
        
    return lead_timestamps, leads


@composable_relational
@cache_manager.create_cache('heading_in_relative_direction_to')
def heading_in_relative_direction_to(track_candidates, related_candidates, log_dir, direction:Literal['same', 'opposite', 'perpendicular']):
    """Returns the subset of track candidates that are traveling in the given direction compared to the related canddiates.

    Arguements:
        track_candidates: The set of objects that could be traveling in the given direction
        related_candidates: The set of objects that the direction is relative to
        log_dir: The path to the log data
        direction: The direction that the positive tracks are traveling in relative to the related candidates
            "opposite" indicates the track candidates are traveling in a direction 135-180 degrees from the direction the related candidates
            are heading toward.
            "same" indicates the track candidates that are traveling in a direction 0-45 degrees from the direction the related candiates
            are heading toward.
            "same" indicates the track candidates that are traveling in a direction 45-135 degrees from the direction the related candiates
            are heading toward.

    Returns:
        the subset of track candidates that are traveling in the given direction compared to the related candidates.

    Example:
        oncoming_traffic = heading_in_relative_direction_to(vehicles, ego_vehicle, log_dir, direction='opposite')    
    """
    track_uuid = track_candidates

    track_pos, _ = get_nth_pos_deriv(track_uuid, 0, log_dir)
    track_vel, track_timestamps = get_nth_pos_deriv(track_uuid, 1, log_dir)

    traveling_in_direction_timestamps = []
    traveling_in_direction_objects = {}
    ego_to_city = get_ego_SE3(log_dir)

    for related_uuid in related_candidates:
        if track_uuid == related_uuid:
            continue
        
        related_pos, _ = get_nth_pos_deriv(related_uuid, 0, log_dir)
        related_vel, related_timestamps = get_nth_pos_deriv(related_uuid, 1, log_dir)
        for i, timestamp in enumerate(track_timestamps):

            if timestamp in related_timestamps:

                track_dir = track_vel[i]
                related_dir = related_vel[list(related_timestamps).index(timestamp)]

                if np.linalg.norm(track_dir) < 1 and has_free_will(track_uuid,log_dir) and np.linalg.norm(related_dir) > 1:
                    track_cuboid = get_cuboid_from_uuid(track_uuid, log_dir, timestamp=timestamp)
                    track_self_dir = np.array([1,0,0])

                    timestamp_track_pos = track_pos[i]
                    timestamp_track_posx = ego_to_city[timestamp].compose(track_cuboid.dst_SE3_object).transform_from(track_self_dir)
                    track_dir = timestamp_track_posx - timestamp_track_pos

                elif np.linalg.norm(related_dir) < 1 and has_free_will(related_uuid,log_dir) and np.linalg.norm(track_dir) > .5:
                    related_cuboid = get_cuboid_from_uuid(related_uuid, log_dir, timestamp=timestamp)
                    related_x_dir = np.array([1,0,0])
                    timestamp_related_pos = related_pos[list(related_timestamps).index(timestamp)]
                    timestamp_related_posx = ego_to_city[timestamp].compose(related_cuboid.dst_SE3_object).transform_from(related_x_dir)
                    related_dir = timestamp_related_posx - timestamp_related_pos

                elif np.linalg.norm(track_dir) < 1 or np.linalg.norm(related_dir) < 1:
                    continue
                
                track_dir = track_dir/np.linalg.norm(track_dir + 1e-8)
                related_dir = related_dir/np.linalg.norm(related_dir + 1e-8)
                angle = np.rad2deg(np.arccos(np.dot(track_dir, related_dir)))

                if (angle <= 45 and direction == 'same'
                or 45 < angle < 135 and direction == 'perpendicular'
                or 135 <= angle < 180 and direction == 'opposite'):
                    if related_uuid not in traveling_in_direction_objects:
                        traveling_in_direction_objects[related_uuid] = []
                    traveling_in_direction_objects[related_uuid].append(timestamp)
                    traveling_in_direction_timestamps.append(timestamp)

    return traveling_in_direction_timestamps, traveling_in_direction_objects


@composable
@cache_manager.create_cache('stationary')
def stationary(track_candidates:dict, log_dir:Path):
    """
    Returns objects that moved less than 2m over their length of observation in the scneario.
    This object is only intended to separate parked from active vehicles. 
    Use has_velocity() with thresholding if you want to indicate vehicles that are temporarily stopped.

    Args:
        track_candidates: Tracks to analyze (scenario dictionary).
        log_dir: Path to scenario logs.

    Returns:
        dict: 
            A filtered scenario dictionary where keys are track UUIDs and values are lists of timestamps when the object is stationary.

    Example:
        parked_vehicles = stationary(vehicles, log_dir)
    """
    track_uuid = track_candidates

    #Displacement threshold needed because of annotation jitter
    DISPLACMENT_THRESH = 3

    pos, timestamps = get_nth_pos_deriv(track_uuid, 0, log_dir)

    max_displacement = np.max(pos, axis=0) - np.min(pos, axis=0)

    if np.linalg.norm(max_displacement) < DISPLACMENT_THRESH:
        return list(timestamps)
    else:
        return []


@cache_manager.create_cache('at_stop_sign')
def at_stop_sign(track_candidates:dict, log_dir:Path, forward_thresh:float=10):
    """
    Identifies timestamps when a tracked object is in a lane corresponding to a stop sign. The tracked
    object must be within 15m of the stop sign. This may highlight vehicles using street parking near a stopped sign.

    Args:
        track_candidates: Tracks to analyze (scenario dictionary).
        log_dir: Path to scenario logs.
        forward_thresh: Distance in meters the vehcile is from the stop sign in the stop sign's front direction

    Returns:
        dict: 
            A filtered scenario dictionary where keys are track UUIDs and values are lists of timestamps when the object is at a stop sign.

    Example:
        vehicles_at_stop_sign = at_stop_sign(vehicles, log_dir)
    """

    stop_sign_uuids = get_uuids_of_category(log_dir, 'STOP_SIGN')
    return at_stop_sign_(track_candidates, stop_sign_uuids, log_dir, forward_thresh=forward_thresh)


@composable
@cache_manager.create_cache('in_drivable_area')
def in_drivable_area(track_candidates:dict, log_dir:Path)->dict:
    """
    Identifies objects within track_candidates that are within a drivable area.

    Args:
        track_candidates: Tracks to analyze (scenario dictionary).
        log_dir: Path to scenario logs.

    Returns:
        dict: 
            A filtered scenario dictionary where keys are track UUIDs and values are lists of timestamps when the object is in a drivable area.

    Example:
        buses_in_drivable_area = in_drivable_area(buses, log_dir)
    """
    track_uuid = track_candidates

    avm = get_map(log_dir)
    pos, timestamps = get_nth_pos_deriv(track_uuid, 0, log_dir)

    drivable_timestamps = []
    drivable_areas = avm.get_scenario_vector_drivable_areas()

    for i in range(len(timestamps)):
        for da in drivable_areas:
            if is_point_in_polygon(pos[i, :2], da.xyz[:,:2]):
                drivable_timestamps.append(timestamps[i])
                break

    return drivable_timestamps


@composable 
@cache_manager.create_cache('on_road')
def on_road(
    track_candidates:dict, 
    log_dir:Path)->dict:
    """
    Identifies objects that are on a road or bike lane. 
    This function should be used in place of in_driveable_area() when referencing objects that are on a road. 
    The road does not include parking lots or other driveable areas connecting the road to parking lots.

    Args:
        track_candidates: Tracks to filter (scenario dictionary).
        log_dir: Path to scenario logs.

    Returns:
    The subset of the track candidates that are currently on a road.

    Example:
        animals_on_road = on_road(animals, log_dir)   
    """

    timestamps = []
    lanes_keyed_by_timetamp = get_scenario_lanes(track_candidates, log_dir)
    
    for timestamp, lanes in lanes_keyed_by_timetamp.items():
        if lanes is not None:
            timestamps.append(timestamp)

    return timestamps


@composable_relational
@cache_manager.create_cache('in_same_lane')
def in_same_lane(
    track_candidates:dict,
    related_candidates:dict, 
    log_dir:Path) -> dict:
    """"
    Identifies tracks that are in the same road lane as a related candidate. 

    Args:
        track_candidates: Tracks to filter (scenario dictionary)
        related_candidates: Potential objects that could be in the same lane as the track (scenario dictionary)
        log_dir: Path to scenario logs.

    Returns:
        dict: 
            A filtered scenario dictionary where keys are track UUIDs and values are lists of timestamps when the object is on a road lane.

    Example:
        bicycle_in_same_lane_as_vehicle = in_same_lane(bicycle, regular_vehicle, log_dir)    
    """

    track_uuid = track_candidates
    avm = get_map(log_dir)
    track_ls = get_scenario_lanes(track_uuid, log_dir, avm=avm)
    semantic_lanes = {timestamp:get_semantic_lane(ls, log_dir, avm=avm) for timestamp, ls in track_ls.items()}
    timestamps = track_ls.keys()

    same_lane_timestamps = []
    sharing_lanes = {}

    for i, related_uuid in enumerate(related_candidates):

        if related_uuid == track_uuid:
            continue

        related_ls = get_scenario_lanes(related_uuid, log_dir, avm=avm)

        for timestamp in timestamps:
            if (timestamp in related_ls and related_ls[timestamp] is not None and 
            related_ls[timestamp] in semantic_lanes[timestamp]):
                if related_uuid not in sharing_lanes:
                    sharing_lanes[related_uuid] = []
                
                same_lane_timestamps.append(timestamp)
                sharing_lanes[related_uuid].append(timestamp)

    return same_lane_timestamps, sharing_lanes


@composable_relational
@cache_manager.create_cache('on_relative_side_of_road')
def on_relative_side_of_road(
    track_candidates:dict,
    related_candidates:dict, 
    log_dir:Path,
    side=Literal['same', 'opposite']) -> dict:
    """"
    Identifies tracks that are in the same road lane as a related candidate. 

    Args:
        track_candidates: Tracks to filter (scenario dictionary)
        related_candidates: Potential objects that could be in the same lane as the track (scenario dictionary)
        log_dir: Path to scenario logs.

    Returns:
        dict: 
            A filtered scenario dictionary where keys are track UUIDs and values are lists of timestamps when the object is on a road lane.

    Example:
        bicycle_in_same_lane_as_vehicle = in_same_lane(bicycle, regular_vehicle, log_dir)    
    """

    track_uuid = track_candidates
    traj, timestamps = get_nth_pos_deriv(track_uuid, 0, log_dir)

    avm = get_map(log_dir)
    track_ls = get_scenario_lanes(track_uuid, log_dir, avm=avm)
    semantic_lanes = {timestamp:get_road_side(track_ls[timestamp], log_dir, side=side, avm=avm) for timestamp in timestamps}

    same_lane_timestamps = []
    sharing_lanes = {}

    for i, related_uuid in enumerate(related_candidates):

        if related_uuid == track_uuid:
            continue

        related_ls = get_scenario_lanes(related_uuid, log_dir, avm=avm)

        for timestamp in timestamps:
            if (timestamp in related_ls and related_ls[timestamp] is not None and 
            related_ls[timestamp] in semantic_lanes[timestamp]):
                if related_uuid not in sharing_lanes:
                    sharing_lanes[related_uuid] = []
                
                same_lane_timestamps.append(timestamp)
                sharing_lanes[related_uuid].append(timestamp)

    return same_lane_timestamps, sharing_lanes


@cache_manager.create_cache('scenario_and')
def scenario_and(scenario_dicts:list[dict])->dict:
    """
    Returns a composed scenario where the track objects are the intersection of all of the track objects
    with the same uuid and timestamps.

    Args:
        scenario_dicts: the scenarios to combine 

    Returns:
        dict:
            a filtered scenario dictionary that contains tracked objects found in all given scenario dictionaries
    
    Example:
        jaywalking_peds = scenario_and([peds_on_road, peds_not_on_pedestrian_crossing])

    """
    composed_dict = {}

    composed_track_dict = deepcopy(reconstruct_track_dict(scenario_dicts[0]))
    for i in range(1, len(scenario_dicts)):
        scenario_dict = scenario_dicts[i]
        track_dict = reconstruct_track_dict(scenario_dict)
        
        for track_uuid, timestamps in track_dict.items():
            if track_uuid not in composed_track_dict:
                continue

            composed_track_dict[track_uuid] = sorted(set(composed_track_dict[track_uuid]).intersection(timestamps))

        for track_uuid in list(composed_track_dict.keys()):
            if track_uuid not in track_dict:
                composed_track_dict.pop(track_uuid)

    for track_uuid, intersecting_timestamps in composed_track_dict.items():
        for scenario_dict in scenario_dicts:
            if track_uuid not in composed_dict:
                composed_dict[track_uuid] = scenario_at_timestamps(scenario_dict[track_uuid], intersecting_timestamps)
            else:
                related_children =  scenario_at_timestamps(scenario_dict[track_uuid],intersecting_timestamps)

                if isinstance(related_children, dict) and isinstance(composed_dict[track_uuid], dict):
                    composed_dict[track_uuid] = scenario_or([composed_dict[track_uuid], related_children])
                elif isinstance(related_children, dict) and not isinstance(composed_dict[track_uuid], dict):
                    related_children[track_uuid] = composed_dict[track_uuid]
                    composed_dict[track_uuid] = related_children
                elif not isinstance(related_children, dict) and isinstance(composed_dict[track_uuid], dict):
                    composed_dict[track_uuid][track_uuid] = related_children
                else:
                    composed_dict[track_uuid] = set(composed_dict[track_uuid]).intersection(related_children)

    return composed_dict


@cache_manager.create_cache('scenario_or')
def scenario_or(scenario_dicts:list[dict]):
    """
    Returns a composed scenario where that tracks all objects and relationships in all of the input scenario dicts.

    Args:
        scenario_dicts: the scenarios to combine 

    Returns:
        dict:
            an expanded scenario dictionary that contains every tracked object in the given scenario dictionaries
    
    Example:
        be_cautious_around = scenario_or([animal_on_road, stroller_on_road])
    """

    composed_dict = deepcopy(scenario_dicts[0])
    for i in range(1, len(scenario_dicts)):
        for track_uuid, child in scenario_dicts[i].items():
            if track_uuid not in composed_dict:
                composed_dict[track_uuid] = child
            elif isinstance(child, dict) and isinstance(composed_dict[track_uuid], dict):
                composed_dict[track_uuid] = scenario_or([composed_dict[track_uuid], child])
            elif isinstance(child, dict) and not isinstance(composed_dict[track_uuid], dict):
                child[track_uuid] = composed_dict[track_uuid]
                composed_dict[track_uuid] = child
            elif not isinstance(child, dict) and isinstance(composed_dict[track_uuid], dict):
                composed_dict[track_uuid][track_uuid] = child
            else:
                composed_dict[track_uuid] = set(composed_dict[track_uuid]).union(child)

    return composed_dict


def reverse_relationship(func):
    """
    Wraps relational functions to switch the top level tracked objects and relationships formed by the function. 

    Args:
        relational_func: Any function that takes track_candidates and related_candidates as its first and second arguements

    Returns:
        dict:
            scenario dict with swapped top-level tracks and related candidates

    Example:
        group_of_peds_near_vehicle = reverse_relationship(near_objects)(vehicles, peds, log_dir, min_objects=3)
    """
    def wrapper(track_candidates, related_candidates, log_dir, *args, **kwargs):

        if func.__name__ == 'get_objects_in_relative_direction':
            return has_objects_in_relative_direction(track_candidates, related_candidates, log_dir, *args, **kwargs)

        track_dict = to_scenario_dict(track_candidates, log_dir)
        related_dict = to_scenario_dict(related_candidates, log_dir)
        remove_empty_branches(track_dict)
        remove_empty_branches(related_dict)

        scenario_dict:dict = func(track_dict, related_dict, log_dir, *args, **kwargs)
        remove_empty_branches(scenario_dict)

        #Look for new relationships
        tc_uuids = list(track_dict.keys())
        rc_uuids = list(related_dict.keys())

        new_relationships = []
        for track_uuid, related_objects in scenario_dict.items():
            for related_uuid in related_objects.keys():
                if track_uuid in tc_uuids and related_uuid in rc_uuids \
                or track_uuid in rc_uuids and related_uuid in tc_uuids \
                and track_uuid != related_uuid:
                    new_relationships.append((track_uuid, related_uuid))

        #Reverese the scenario dict using these new relationships
        reversed_scenario_dict = {}
        for track_uuid, related_uuid in new_relationships:
            related_timestamps = get_scenario_timestamps(scenario_dict[track_uuid][related_uuid])
            removed_related:dict = deepcopy(scenario_dict[track_uuid])

            # I need a new data structure
            for track_uuid2, related_uuid2 in new_relationships:
                if track_uuid2 == track_uuid:
                    removed_related.pop(related_uuid2)

            if len(removed_related) == 0 or len(get_scenario_timestamps(removed_related)) == 0:
                removed_related = related_timestamps

            filtered_removed_related = scenario_at_timestamps(removed_related, related_timestamps)
            filtered_removed_related = {track_uuid : filtered_removed_related}

            if related_uuid not in reversed_scenario_dict:
                reversed_scenario_dict[related_uuid] = filtered_removed_related
            else:
                reversed_scenario_dict[related_uuid] = scenario_or([filtered_removed_related, reversed_scenario_dict[related_uuid]])

        return reversed_scenario_dict
    return wrapper


def scenario_not(func):
    """
    Wraps composable functions to return the difference of the input track dict and output scenario dict. 
    Using scenario_not with a composable relational function will not return any relationships. 

    Args:
        composable_func: Any function that takes track_candidates as its first input

    Returns:

    Example:
        active_vehicles = scenario_not(stationary)(vehicles, log_dir)
    """
    def wrapper(track_candidates, *args, **kwargs):

        sig = inspect.signature(func)
        params = list(sig.parameters.keys())
        
        # Determine the position of 'log_dir'
        if 'log_dir' in params:
            log_dir_index = params.index('log_dir') - 1
        else:
            raise ValueError("The function does not have 'log_dir' as a parameter.")

        log_dir = args[log_dir_index]

        if func.__name__ == 'get_objects_in_relative_direction':
            track_dict = to_scenario_dict(args[0], log_dir)
        else:
            track_dict = to_scenario_dict(track_candidates, log_dir)

        if log_dir_index == 0:
            scenario_dict = func(track_candidates, log_dir, *args[1:], **kwargs)
        elif log_dir_index == 1:
            #composable_relational function
            scenario_dict = func(track_candidates, args[0], log_dir, *args[2:], **kwargs)

        remove_empty_branches(scenario_dict)
        not_dict = {track_uuid: [] for track_uuid in track_dict.keys()}

        for uuid in not_dict:
            if uuid in scenario_dict:
                not_timestamps = list(set(
                    get_scenario_timestamps(track_dict[uuid])).difference(get_scenario_timestamps(scenario_dict[uuid])))
                
                not_dict[uuid] = scenario_at_timestamps(track_dict[uuid], not_timestamps)
            else:
                not_dict[uuid] = track_dict[uuid]

        return not_dict
    return wrapper


def output_scenario(
    scenario:dict,
    description:str, 
    log_dir:Path, 
    output_dir:Path, 
    visualize:bool=False, 
    **visualization_kwargs):
    """
    Outputs a file containing the predictions in an evaluation-ready format. Do not provide any visualization kwargs. 
    """

    prediction_path = output_dir / log_dir.name / f'{description}_{log_dir.name[:8]}_.pkl'
    prediction_path.unlink(missing_ok=True)

    post_process_scenario(scenario, log_dir)

    Path(output_dir/log_dir.name).mkdir(parents=True, exist_ok=True)
    create_mining_pkl(description, scenario, log_dir, output_dir)

    if visualize:
        # PyVista and VTK can be a headache to set up on your machine. If this is the case,
        # set visualization to false
        from refAV.visualization import visualize_scenario

        log_scenario_visualization_path = Path(output_dir/log_dir.name/'scenario visualizations')
        log_scenario_visualization_path.mkdir(exist_ok=True)

        for file in log_scenario_visualization_path.iterdir():
            if file.is_file() and file.stem.split(sep='_')[0] == description:
                file.unlink()

        visualize_scenario(scenario, log_dir, log_scenario_visualization_path, description=description, **visualization_kwargs)

    

